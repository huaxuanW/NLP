{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(layer):\n",
    "    \"\"\"\n",
    "    it is necessary to init weight using some distribution, \n",
    "    since the default initialized weights from `torch.tensor` can be in large range,\n",
    "    which may cause the model hard to converge.\n",
    "    \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "    if layer.bias is not None:\n",
    "        nn.init.constant_(layer.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, dk, mask, dropout):\n",
    "    # scale inplace\n",
    "    q.mul_(dk ** -0.5)\n",
    "    \n",
    "    # calculate similarity score and softmax\n",
    "    score = torch.matmul(q, k.transpose(-2, -1)) # q:(batch_size, num_head, seqlen, dk) k.T: (batch_size, num_head, dk, seqlen) -> score:(batch_size, num_head, seqlen, seqlen)\n",
    "    score = torch.softmax(score, dim=-1) # calculate probability on the last dimension\n",
    "\n",
    "    # apply mask inplace\n",
    "    if mask is not None:\n",
    "        score.masked_fill_(mask.unsqueeze(1), -1e9)\n",
    "    # apply dropout\n",
    "    if dropout is not None:\n",
    "        score = dropout(score)\n",
    "    # apply similarity score on value\n",
    "    result = torch.matmul(score, v) # score:(batch_size, num_head, seqlen, seqlen), v:(batch_size, num_head, seqlen, dk) -> result:(batch_size, num_head, seqlen, dk)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sub-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    implement point-wise Feed Forward sub-layer in transformer\n",
    "\n",
    "    point-wise == apply same fc for each word\n",
    "\n",
    "    map the extracted feature into the desired semantic space\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs_dim=512, hidden_dim=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(inputs_dim, hidden_dim,)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim, inputs_dim,)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        init_weight(self.fc1)\n",
    "        init_weight(self.fc2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    implement Multi-head attention sub-layer in transformer\n",
    "\n",
    "    purpose: extract sequential feature that is needed from input sequence\n",
    "\n",
    "    speed: comparing to RNN that sequentially pass the feature, attention globally extract feature \n",
    "\n",
    "    performance: like multi-channel CNN, multi-head attention extract different feature pattern \n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, dropout=0.1, num_head=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dk = embedding_dim // num_head #embedding_dim need to be greater than num_head\n",
    "        self.num_head = num_head\n",
    "        \n",
    "        self.query = nn.Linear(embedding_dim, self.dk * num_head, bias=False)\n",
    "        self.key = nn.Linear(embedding_dim, self.dk * num_head, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, self.dk * num_head, bias=False)\n",
    "\n",
    "        init_weight(self.query)\n",
    "        init_weight(self.key)\n",
    "        init_weight(self.value)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.output_linear = nn.Linear(self.dk * num_head, embedding_dim, bias=False)\n",
    "\n",
    "        init_weight(self.output_linear)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into N heads\n",
    "        q = self.query(q).view(batch_size, -1, self.num_head, self.dk)\n",
    "        k = self.key(k).view(batch_size, -1, self.num_head, self.dk)\n",
    "        v = self.value(v).view(batch_size, -1, self.num_head, self.dk)\n",
    "\n",
    "        # transpose from (batch, seqlen, num_head, dk) -> (batch, num_head, seqlen, dk)\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        #calculate attention\n",
    "        att_res = attention(q, k, v, self.dk, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads \n",
    "        # (batch, num_head, seqlen, dk) -> (batch, seqlen, num_head, dk) -> (batch, seqlen, dk*num_head)\n",
    "        att_res = att_res.transpose(1, 2).contiguous().view(batch_size, -1, self.dk*self.num_head)\n",
    "        \n",
    "        # pass though output linear layer\n",
    "        att_res = self.output_linear(att_res)\n",
    "\n",
    "        return att_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout=0.1, num_head=8, hidden_dim=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.att = MultiHeadAttention(embedding_dim, dropout=dropout, num_head=num_head)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = FeedForward(inputs_dim=embedding_dim,hidden_dim=hidden_dim, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x1 = self.att(x, x, x, mask)\n",
    "        x1 = self.dropout1(x1) + x\n",
    "        x1 = self.norm1(x1)\n",
    "        \n",
    "        x2 = self.fc(x1)\n",
    "        x2 = self.dropout2(x2) + x1\n",
    "        x2 = self.norm2(x2)\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout=0.1, num_head=8, hidden_dim=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.att1 = MultiHeadAttention(embedding_dim, dropout=dropout, num_head=num_head)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.att2 = MultiHeadAttention(embedding_dim, dropout=dropout, num_head=num_head)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = FeedForward(inputs_dim=embedding_dim,hidden_dim=hidden_dim, dropout=dropout)\n",
    "        self.norm3 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encode_output, x_mask, output_mask):\n",
    "        x1 = self.att1(x, x, x, x_mask)\n",
    "        x1 = self.dropout1(x1) + x\n",
    "        x1 = self.norm1(x1)\n",
    "\n",
    "        x2 = self.att2(k=x1, q=encode_output, v=encode_output, mask=output_mask)\n",
    "        x2 = self.dropout2(x2) + x1\n",
    "        x2 = self.norm2(x2)\n",
    "        \n",
    "        x3 = self.fc(x2)\n",
    "        x3 = self.dropout2(x3) + x2\n",
    "        x3 = self.norm3(x3)\n",
    "\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, block, num_block, norm=None):\n",
    "        super().__init__()\n",
    "        self.blocks = _get_clones(block, num_block)\n",
    "        self.N = num_block\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        output = x\n",
    "        for block in self.blocks:\n",
    "            output = block(output, mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, block, num_block, norm=None):\n",
    "        super().__init__()\n",
    "        self.blocks = _get_clones(block, num_block)\n",
    "        self.N = num_block\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        output = x\n",
    "        for block in self.blocks:\n",
    "            output = block(output, mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        \"\"\"\n",
    "        N: number of encoder/decoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch, num_head, sentence_length, embedding_dim = 1, 8, 5, 3\n",
    "# q = v = k = torch.randn(batch, num_head, sentence_length, embedding_dim)\n",
    "\n",
    "batch, sentence_length, embedding_dim = 10, 6, 100\n",
    "q = torch.randn(batch, sentence_length, embedding_dim)\n",
    "\n",
    "model = decoder_block(embedding_dim)\n",
    "\n",
    "res = model(q, q, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6503862c40d495a2e133e85b56abeee686ff746d940c7807a3cb12ae6fc32603"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
